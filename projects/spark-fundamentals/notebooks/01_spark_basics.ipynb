{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3cac5a64-f4ab-4d03-8ba2-80cc1d77a0ea",
   "metadata": {},
   "source": [
    "# Fundamentos do Apache Spark\n",
    "\n",
    "Este notebook introduz os conceitos fundamentais do Apache Spark, demonstrando como criar e operar em RDDs (Resilient Distributed Datasets) e DataFrames.\n",
    "\n",
    "## Tópicos Abordados\n",
    "\n",
    "1. Configuração do ambiente Spark\n",
    "2. Manipulação de RDDs\n",
    "3. Introdução a SparkSQL e DataFrames\n",
    "4. Operações de transformação e ação\n",
    "5. Conceitos de particionamento e persistência\n",
    "6. Visualização do plano de execução"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86ca8f05-55a0-4aee-a8e0-a539d553c8c0",
   "metadata": {},
   "source": [
    "## 1. Configuração e Inicialização da Sessão Spark\n",
    "\n",
    "O SparkSession é o ponto de entrada para a programação Spark com as APIs Dataset e DataFrame. Em um ambiente PySpark, a sessão geralmente já está disponível como variável `spark`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e8b0b3-a4a9-4ae2-9c5a-23e6ea4f3a63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verificamos se a sessão spark está disponível\n",
    "try:\n",
    "    spark\n",
    "except NameError:\n",
    "    # Se não estiver, criamos uma nova\n",
    "    from pyspark.sql import SparkSession\n",
    "    \n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"SparkFundamentals\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "        .config(\"spark.executor.memory\", \"1g\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Obtemos o SparkContext a partir da sessão\n",
    "sc = spark.sparkContext\n",
    "\n",
    "# Exibimos informações da aplicação\n",
    "print(f\"Versão do Spark: {spark.version}\")\n",
    "print(f\"Aplicação: {sc.appName}\")\n",
    "print(f\"Master: {sc.master}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db08e78-7c9e-44fe-9a0a-7f903ffbb16e",
   "metadata": {},
   "source": [
    "## 2. RDDs (Resilient Distributed Datasets)\n",
    "\n",
    "RDDs são a estrutura de dados fundamental do Spark. Eles representam uma coleção distribuída imutável de objetos que podem ser processados em paralelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5a3cf30-43be-4cce-9a8d-5bb371c9dd55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um RDD a partir de uma coleção\n",
    "numbers = range(1, 1001)\n",
    "numbers_rdd = sc.parallelize(numbers, numSlices=4)  # 4 partições\n",
    "\n",
    "# Verificando o número de partições\n",
    "print(f\"Número de partições: {numbers_rdd.getNumPartitions()}\")\n",
    "\n",
    "# Exibindo os primeiros 5 elementos\n",
    "print(f\"Primeiros 5 elementos: {numbers_rdd.take(5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80842ece-b246-401f-a0ee-b87bfd3c1eec",
   "metadata": {},
   "source": [
    "### 2.1 Operações de Transformação em RDDs\n",
    "\n",
    "Transformações criam um novo RDD a partir de um existente. Elas são lazy (preguiçosas), ou seja, só são executadas quando uma ação é solicitada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3edc19-90a9-4aa8-a2e7-ef8c0a9bea2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformação: map\n",
    "squared_rdd = numbers_rdd.map(lambda x: x * x)\n",
    "\n",
    "# Transformação: filter\n",
    "even_squares_rdd = squared_rdd.filter(lambda x: x % 2 == 0)\n",
    "\n",
    "# Neste ponto, nenhuma computação foi realizada ainda devido à avaliação lazy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a27eaea-aa4a-49b0-9c95-14e34edf64e3",
   "metadata": {},
   "source": [
    "### 2.2 Operações de Ação em RDDs\n",
    "\n",
    "Ações retornam valores para o driver ou escrevem dados em um sistema de armazenamento. Elas acionam a execução das transformações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8da99f-3c1a-49f5-a9fe-6f40a9c4c499",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ação: count\n",
    "count = even_squares_rdd.count()\n",
    "print(f\"Número de quadrados pares: {count}\")\n",
    "\n",
    "# Ação: take (retorna os primeiros n elementos)\n",
    "first_10 = even_squares_rdd.take(10)\n",
    "print(f\"Primeiros 10 quadrados pares: {first_10}\")\n",
    "\n",
    "# Ação: collect (traz todos os elementos para o driver - cuidado com grandes conjuntos de dados!)\n",
    "# Usar apenas para conjuntos pequenos ou amostragem\n",
    "sample = even_squares_rdd.sample(False, 0.01).collect()\n",
    "print(f\"Amostra de {len(sample)} elementos: {sample[:5]}...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fead1ade-4cdd-46f5-88b9-2b45d8b3b50c",
   "metadata": {},
   "source": [
    "### 2.3 Operações mais complexas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4bde83-b3f8-49a4-82c9-25d02a33aece",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um RDD com pares (chave, valor)\n",
    "pairs_rdd = numbers_rdd.map(lambda x: (x % 10, x))  # chave é o resto da divisão por 10\n",
    "\n",
    "# GroupByKey - agrupa valores pela chave\n",
    "grouped = pairs_rdd.groupByKey()\n",
    "result = grouped.map(lambda x: (x[0], list(x[1]))).collect()\n",
    "print(\"Agrupamento por chave (primeiros 3):\")\n",
    "for i in range(min(3, len(result))):\n",
    "    key, values = result[i]\n",
    "    print(f\"  Chave {key}: {values[:5]}...\")\n",
    "\n",
    "# ReduceByKey - mais eficiente que groupByKey para agregações\n",
    "sums = pairs_rdd.reduceByKey(lambda a, b: a + b)\n",
    "print(\"\\nSoma dos valores por chave:\")\n",
    "for key, value in sorted(sums.collect()):\n",
    "    print(f\"  Chave {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1d41797-f1bc-460c-b2a9-e2598d1d07e8",
   "metadata": {},
   "source": [
    "### 2.4 Persistência (caching)\n",
    "\n",
    "A persistência permite armazenar RDDs na memória ou no disco para reutilização em várias ações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6a4a02-2d0e-4c48-8a82-ffa7e95de5fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import StorageLevel\n",
    "\n",
    "# Criamos um RDD que será usado várias vezes\n",
    "expensive_rdd = numbers_rdd.map(lambda x: (x, x**3))\n",
    "\n",
    "# Persistimos o RDD na memória\n",
    "expensive_rdd.persist(StorageLevel.MEMORY_AND_DISK)\n",
    "\n",
    "# Primeira ação (executará a transformação)\n",
    "import time\n",
    "start = time.time()\n",
    "count1 = expensive_rdd.count()\n",
    "end = time.time()\n",
    "print(f\"Primeira contagem: {count1} (tempo: {end-start:.4f}s)\")\n",
    "\n",
    "# Segunda ação (usará a versão em cache)\n",
    "start = time.time()\n",
    "count2 = expensive_rdd.count()\n",
    "end = time.time()\n",
    "print(f\"Segunda contagem: {count2} (tempo: {end-start:.4f}s)\")\n",
    "\n",
    "# Não esqueça de liberar a memória quando não precisar mais do RDD em cache\n",
    "expensive_rdd.unpersist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89af0b4d-04ac-48d4-905e-5ebb9b63f7c1",
   "metadata": {},
   "source": [
    "## 3. DataFrames e SparkSQL\n",
    "\n",
    "DataFrames são conjuntos de dados distribuídos organizados em colunas nomeadas, similares a tabelas em bancos de dados relacionais."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9c05f0d-1c38-4e6a-8c5e-4b3ab07db9c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando um DataFrame a partir de uma lista de dados\n",
    "from pyspark.sql import Row\n",
    "\n",
    "# Dados de exemplo: vendas de produtos\n",
    "sales_data = [\n",
    "    Row(date=\"2025-01-15\", product=\"Laptop\", category=\"Electronics\", price=1200, quantity=5),\n",
    "    Row(date=\"2025-01-15\", product=\"Mouse\", category=\"Electronics\", price=25, quantity=30),\n",
    "    Row(date=\"2025-01-16\", product=\"Monitor\", category=\"Electronics\", price=350, quantity=10),\n",
    "    Row(date=\"2025-01-16\", product=\"Desk Chair\", category=\"Furniture\", price=175, quantity=8),\n",
    "    Row(date=\"2025-01-17\", product=\"Coffee Maker\", category=\"Appliances\", price=80, quantity=12),\n",
    "    Row(date=\"2025-01-17\", product=\"Sofa\", category=\"Furniture\", price=950, quantity=2),\n",
    "    Row(date=\"2025-01-18\", product=\"Blender\", category=\"Appliances\", price=70, quantity=15),\n",
    "    Row(date=\"2025-01-18\", product=\"Headphones\", category=\"Electronics\", price=120, quantity=20),\n",
    "    Row(date=\"2025-01-19\", product=\"Dining Table\", category=\"Furniture\", price=600, quantity=3),\n",
    "    Row(date=\"2025-01-19\", product=\"Smartphone\", category=\"Electronics\", price=800, quantity=10)\n",
    "]\n",
    "\n",
    "# Criar DataFrame\n",
    "sales_df = spark.createDataFrame(sales_data)\n",
    "\n",
    "# Exibir o schema (estrutura) do DataFrame\n",
    "sales_df.printSchema()\n",
    "\n",
    "# Mostrar os primeiros registros\n",
    "sales_df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dbe98e-e909-40dd-930e-f04dba0e08c2",
   "metadata": {},
   "source": [
    "### 3.1 Operações básicas com DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ca5ea3b-1de5-4ee6-a54e-7e8ae2bb2ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleção de colunas\n",
    "sales_df.select(\"date\", \"product\", \"price\").show(5)\n",
    "\n",
    "# Filtragem\n",
    "sales_df.filter(sales_df.category == \"Electronics\").show()\n",
    "\n",
    "# Filtragem com expressão SQL\n",
    "sales_df.filter(\"price > 500\").show()\n",
    "\n",
    "# Ordenação\n",
    "sales_df.orderBy(sales_df.price.desc()).show(5)\n",
    "\n",
    "# Adicionando colunas calculadas\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "sales_df.withColumn(\"total_value\", col(\"price\") * col(\"quantity\")) \\\n",
    "        .withColumn(\"tax\", col(\"price\") * col(\"quantity\") * lit(0.1)) \\\n",
    "        .select(\"product\", \"quantity\", \"price\", \"total_value\", \"tax\") \\\n",
    "        .show(5)\n",
    "\n",
    "# Funções de agregação\n",
    "from pyspark.sql.functions import sum, avg, count, max, min\n",
    "\n",
    "sales_df.groupBy(\"category\") \\\n",
    "        .agg(count(\"product\").alias(\"product_count\"), \n",
    "             sum(\"price\").alias(\"total_price\"), \n",
    "             avg(\"price\").alias(\"avg_price\")) \\\n",
    "        .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71f56d0-ea16-40e9-bc99-7d7e72cca76a",
   "metadata": {},
   "source": [
    "### 3.2 Usando SQL com DataFrames\n",
    "\n",
    "O Spark permite executar consultas SQL em DataFrames registrados como tabelas temporárias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2e05fbe-0c25-44fa-b2d7-b71ec96f89c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Registrar DataFrame como uma tabela temporária\n",
    "sales_df.createOrReplaceTempView(\"sales\")\n",
    "\n",
    "# Executar uma consulta SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category, \n",
    "        COUNT(*) as product_count,\n",
    "        SUM(price * quantity) as total_revenue,\n",
    "        AVG(price) as avg_price\n",
    "    FROM sales\n",
    "    GROUP BY category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "result.show()\n",
    "\n",
    "# Consulta mais complexa\n",
    "daily_sales = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        date,\n",
    "        SUM(price * quantity) as daily_revenue,\n",
    "        COUNT(DISTINCT category) as categories_sold,\n",
    "        MAX(price) as most_expensive_item\n",
    "    FROM sales\n",
    "    GROUP BY date\n",
    "    ORDER BY date\n",
    "\"\"\")\n",
    "\n",
    "daily_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8bec8d1-9308-4fb8-b1cd-25f20c9cbce1",
   "metadata": {},
   "source": [
    "### 3.3 Leitura e Escrita de Dados\n",
    "\n",
    "O Spark suporta vários formatos de dados e fontes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49f13c2-34e3-4ac9-af7e-0c7a34055dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Escrever o DataFrame em diferentes formatos\n",
    "\n",
    "# CSV\n",
    "sales_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"/home/jovyan/data/sales_csv\")\n",
    "\n",
    "# Parquet (formato colunar otimizado)\n",
    "sales_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"/home/jovyan/data/sales_parquet\")\n",
    "\n",
    "# JSON\n",
    "sales_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .json(\"/home/jovyan/data/sales_json\")\n",
    "\n",
    "# Ler dados de volta\n",
    "parquet_df = spark.read.parquet(\"/home/jovyan/data/sales_parquet\")\n",
    "parquet_df.show(5)\n",
    "\n",
    "# Ler CSV com opções\n",
    "csv_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/jovyan/data/sales_csv\")\n",
    "    \n",
    "csv_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c69f10-11c8-44a7-b33a-9d8d24cb4618",
   "metadata": {},
   "source": [
    "## 4. Otimização e Plano de Execução\n",
    "\n",
    "O otimizador do Spark (Catalyst) cria um plano de execução otimizado para suas operações."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0f0b33-20f8-4ee9-968d-df54183cab71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma consulta complexa\n",
    "complex_query = sales_df \\\n",
    "    .filter(col(\"price\") > 100) \\\n",
    "    .join(\n",
    "        sales_df.groupBy(\"category\").agg(avg(\"price\").alias(\"avg_category_price\")),\n",
    "        on=\"category\"\n",
    "    ) \\\n",
    "    .filter(col(\"price\") > col(\"avg_category_price\")) \\\n",
    "    .select(\"date\", \"product\", \"category\", \"price\", \"avg_category_price\")\n",
    "\n",
    "# Exibir o plano de execução lógico\n",
    "print(\"\\nPlano Lógico:\")\n",
    "complex_query.explain()\n",
    "\n",
    "# Exibir o plano físico\n",
    "print(\"\\nPlano Físico Otimizado:\")\n",
    "complex_query.explain(\"extended\")\n",
    "\n",
    "# Executar a consulta\n",
    "complex_query.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beaa8ce8-d59b-4622-86e2-66219e8bcc9c",
   "metadata": {},
   "source": [
    "## 5. Particionamento e Performance\n",
    "\n",
    "O particionamento adequado de dados é crucial para a performance em aplicações Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8d2c96-edcf-47f3-b9aa-2fb3fbc12ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar um DataFrame maior para demonstrar particionamento\n",
    "from pyspark.sql.functions import explode, sequence, to_date\n",
    "\n",
    "# Gerar datas para os últimos 30 dias\n",
    "date_df = spark.sql(\"\"\"\n",
    "    SELECT explode(sequence(to_date('2025-01-01'), to_date('2025-01-30'), interval 1 day)) as date\n",
    "\"\"\")\n",
    "\n",
    "# Verificar o número padrão de partições\n",
    "print(f\"Número de partições do SQL Shuffle: {spark.conf.get('spark.sql.shuffle.partitions')}\")\n",
    "\n",
    "# Alterar o número de partições para um valor menor para este exemplo\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"4\")\n",
    "\n",
    "# Exemplo de operação com shuffling\n",
    "large_agg = sales_df \\\n",
    "    .crossJoin(date_df) \\\n",
    "    .groupBy(\"date\", \"category\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"date\", \"category\")\n",
    "\n",
    "# Verificar o plano de execução\n",
    "large_agg.explain()\n",
    "\n",
    "# Executar com o número especificado de partições\n",
    "large_agg.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f05c0ae-0d88-4e52-88ce-0fe7b19e9e1d",
   "metadata": {},
   "source": [
    "### 5.1 Broadcast Join\n",
    "\n",
    "Os broadcast joins podem melhorar significativamente a performance quando uma das tabelas é pequena o suficiente para caber em memória."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2339e1-0d1b-4d2d-b5d3-03c7e66fe50e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar uma tabela de dimensão pequena para categorias\n",
    "category_data = [\n",
    "    Row(category=\"Electronics\", tax_rate=0.08, department=\"Tech\"),\n",
    "    Row(category=\"Furniture\", tax_rate=0.09, department=\"Home\"),\n",
    "    Row(category=\"Appliances\", tax_rate=0.07, department=\"Home\")\n",
    "]\n",
    "category_df = spark.createDataFrame(category_data)\n",
    "\n",
    "# Join normal\n",
    "print(\"\\nJoin normal:\")\n",
    "start = time.time()\n",
    "normal_join = sales_df.join(category_df, on=\"category\")\n",
    "normal_join.explain()\n",
    "normal_join_count = normal_join.count()\n",
    "end = time.time()\n",
    "print(f\"Normal join tempo: {end-start:.4f}s, resultado: {normal_join_count} linhas\")\n",
    "\n",
    "# Broadcast join (explícito)\n",
    "from pyspark.sql.functions import broadcast\n",
    "\n",
    "print(\"\\nBroadcast join explícito:\")\n",
    "start = time.time()\n",
    "broadcast_join = sales_df.join(broadcast(category_df), on=\"category\")\n",
    "broadcast_join.explain()\n",
    "broadcast_join_count = broadcast_join.count()\n",
    "end = time.time()\n",
    "print(f\"Broadcast join tempo: {end-start:.4f}s, resultado: {broadcast_join_count} linhas\")\n",
    "\n",
    "# Mostrar alguns resultados do join\n",
    "broadcast_join.select(\"product\", \"category\", \"price\", \"department\", \"tax_rate\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ac79c2b-e9a9-4950-9908-13c48f3c2fa0",
   "metadata": {},
   "source": [
    "## 6. Conclusão\n",
    "\n",
    "Neste notebook, exploramos os conceitos fundamentais do Apache Spark:\n",
    "\n",
    "1. Criação e manipulação de RDDs\n",
    "2. Trabalho com DataFrames e SparkSQL\n",
    "3. Leitura e escrita de dados em diferentes formatos\n",
    "4. Otimização de consultas e planos de execução\n",
    "5. Técnicas de particionamento e performance\n",
    "\n",
    "Nos próximos notebooks, exploraremos tópicos mais avançados como:\n",
    "- Streaming estruturado\n",
    "- Machine Learning com Spark MLlib\n",
    "- Processamento de dados em grafos com GraphX\n",
    "- Integração com fontes de dados externas\n",
    "\n",
    "Para mais informações, consulte a [documentação oficial do Apache Spark](https://spark.apache.org/docs/latest/)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
